<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>NLP Project 2: insurance reviews</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-opaquegray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}

</style></head><body><article id="e9e87e0c-4ad0-4269-af54-18a78390957a" class="page sans"><header><img class="page-cover-image" src="{{url_for('static', filename='default/report/1000_F_78819928_fwQBpgivBn2yE8z2gejbzGLO1fnrOEot.jpg')}}" style="object-position:center 50%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">💻</span></div><h1 class="page-title">NLP Project 2: insurance reviews</h1></header><div class="page-body"><p id="e47e2a57-d699-4b38-91ac-76f0e2afd86f" class="">Aymeric LEBOUCHER and Gustave ISCLA</p><p id="755806e5-1ca3-4a40-9184-6d840cae35b6" class="">
</p><p id="6156b911-46fe-46e9-b70f-391e08208357" class=""><mark class="highlight-red"><strong>Colab Notebook: </strong></mark><mark class="highlight-blue"><a href="https://colab.research.google.com/drive/1qktMDYOXn7mqvRgx7q8LMpiyCCzOcuAx?usp=sharing">https://colab.research.google.com/drive/1qktMDYOXn7mqvRgx7q8LMpiyCCzOcuAx?usp=sharing</a></mark></p><p id="c655073b-fec3-44c7-a373-d305ccfa89f9" class="">
</p><h1 id="ef685ffa-16d0-4488-bb33-bf328076f8c3" class="">The dataset</h1><p id="b9711d0e-fbe3-4e69-9894-6e742c28b83f" class="">
</p><p id="e3ebb9ec-19b5-4fd8-9c8b-5f97f409fc71" class="">The dataset we had to work with was a csv file containing reviews of insurances’ customers.</p><p id="f22bfd40-f6e9-4fab-a3ab-2c3a72288bac" class="">The reviews contained 6 fields:</p><ul id="fab930d5-1d1a-42dc-8185-9f4cb8981171" class="bulleted-list"><li style="list-style-type:disc">date: the review date and the month of the experience the customer had with the insurance </li></ul><ul id="56595d8b-aebe-4fbb-9c1e-5b15d1e223a5" class="bulleted-list"><li style="list-style-type:disc">note: the rate the customer gave to the insurance</li></ul><ul id="b0ef4b1c-c610-4432-91e9-20d1dc1c99de" class="bulleted-list"><li style="list-style-type:disc">auteur: the customer’s username (not useful for us)</li></ul><ul id="a81e2f05-2e69-44e5-91ea-10372a52be75" class="bulleted-list"><li style="list-style-type:disc">avis: the review itself </li></ul><ul id="46d7b40f-39f0-430d-a380-f0f65024b99c" class="bulleted-list"><li style="list-style-type:disc">assureur: the insurance name </li></ul><ul id="1111e5ba-2afc-4919-903a-88a2d36bf55a" class="bulleted-list"><li style="list-style-type:disc">produit: the insurance product of the customer</li></ul><p id="ca846825-0ef6-43e0-827b-31784c36ed24" class="">
</p><p id="5584961d-9ed3-44b7-a813-e6faa9edee68" class="">Here is what it looked like once imported in with pandas library:</p><figure id="09540642-c758-4308-87b6-76edff500f31" class="image"><a href="{{url_for('static', filename='default/report/Untitled.png')}}"><img style="width:849px" src="{{url_for('static', filename='default/report/Untitled.png')}}"/></a></figure><p id="73853f2b-7603-400b-80ac-00705659c651" class="">
</p><p id="1c924735-1b2a-4ff9-a4e4-5e06712134b3" class="">
</p><h1 id="4ae5c808-d42e-4119-ad6d-55a5691db771" class="">Translate</h1><p id="6c95b814-c52b-4e3e-b2ff-7777134d5a59" class="">
</p><p id="913eba6a-5be7-4a92-9a14-5024ee72ff2a" class="">First, we translated our reviews to english because libraries we had used later were more efficient on english words than french.</p><p id="bfee009b-1c48-4187-8ad5-b9ab27cdf38d" class="">
</p><p id="7ac71d32-39fd-4a01-be59-8e5cbbc9ec60" class="">To do so, we used the GoogleTranslator method from deep_translator library. It provides good translation although maximum text’s length is 5000 characters.</p><p id="b2252aef-245a-4d02-b475-bca93d4a6002" class="">
</p><pre id="52ad523e-ae97-4cbb-9321-a9f4da9a8b7f" class="code"><code>def translate(row):
  if len(row.avis)&gt;=5000:
    return GoogleTranslator(source=&#x27;fr&#x27;, target=&#x27;en&#x27;).translate(text=row.avis[:4999])
  else:
    return GoogleTranslator(source=&#x27;fr&#x27;, target=&#x27;en&#x27;).translate(text=row.avis)</code></pre><p id="71964fba-d707-46b3-a60a-a6abd6396534" class="">
</p><p id="fe509963-338c-4b09-9a09-a31c3552dca6" class="">This process was very slow so we saved the result in a new csv to upload it directly the next times.</p><p id="1c5ddf71-8c6f-4f5e-9f08-ad935747f221" class="">
</p><p id="1de56de1-36fe-49d2-90e2-f7a737ae4660" class="">
</p><h1 id="1fab416c-b499-4612-8c59-51c9bec8c4c8" class="">Data Pre-processing</h1><p id="a53a334a-196f-4b90-bdf0-0720dedfd55c" class="">
</p><p id="9ff99615-e2b5-4bd1-89df-087374a27072" class="">Once we had our translated reviews, we applied some pre-processing functions to clean our data and convert fields in good formats. </p><p id="a38a550d-58fb-49b0-845e-4fa81b87fd61" class="">
</p><h3 id="10f0e7c7-4dd5-47a0-b368-4e65ed7f3467" class="">Dates conversions</h3><p id="013dc708-cc2a-4e12-a93e-10a0f9db4a55" class="">First, we converted the “date” field in order to split review date and experience date for later data exploration. We converted each new column to datetime format.  </p><figure id="e8737105-c8bb-4e8c-bcc6-f2168ff7c54b" class="image" style="text-align:left"><a href="{{url_for('static', filename='default/report/Untitled%201.png')}}"><img style="width:276px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%201.png"/></a></figure><p id="8dae3dd5-f516-4067-b439-1e431253c08a" class="">
</p><h3 id="3829a7f1-736b-47fa-9575-04d108bd7ffa" class=""><strong>Products and insurance categorization</strong></h3><p id="2afdc045-ccbc-41c4-a094-625b4c0974cd" class="">In order to integrate products’ ids and insurances’ ids. We categorized them:</p><pre id="c3bb821e-3905-4b01-b4d6-a767d745ab7d" class="code"><code>train_data[&#x27;product_cat&#x27;]= train_data[&#x27;produit&#x27;].astype(&#x27;category&#x27;).cat.codes.astype(&#x27;int64&#x27;)
train_data[&#x27;assureur_cat&#x27;]= train_data[&#x27;assureur&#x27;].astype(&#x27;category&#x27;).cat.codes.astype(&#x27;int64&#x27;</code></pre><p id="83793b8c-a4db-4404-b9d1-8c7f32a6a960" class="">
</p><h3 id="69754b43-79f0-49c5-8794-d10396dacc6a" class="">Tokenization</h3><p id="d23ba9b8-e24e-4964-a2ff-189ba6486dd7" class="">Then, we tokenized our reviews in order to apply lemmatization later. To do so, we used the TreebankWordTokenizer from nltk library :</p><pre id="34743a30-a665-42c0-a3fc-7cd63c43a085" class="code"><code>tokenizer = TreebankWordTokenizer()

def tokenize(row):
  avis = row.avis.lower()
  words = tokenizer.tokenize(avis)
  row.words = words
  return row

train_data[&quot;words&quot;] = None
train_data = train_data.progress_apply(tokenize, axis=1)</code></pre><p id="7fe9ca76-2c22-4543-9795-06e84e8c5bcd" class="">
</p><h3 id="ef3886db-e01c-4332-8218-ddbfe3fed50d" class="">Special characters removal</h3><p id="ed256e5e-0a24-487e-a19e-0f01596d504e" class="">Because some elements of words lists contained numbers or special characters such as “.”, “!” and so on, we applied a regex function to drop them. </p><pre id="f3d74b24-a7b1-403f-b32d-a11da26ef4ab" class="code"><code>def clean_tokens(row):
  words_without_specials = [word for word in row.words if bool(re.match(&#x27;^[a-zA-Z]*$&#x27;,word))]
  words_without_sw = [word for word in words_without_specials if not word in stopwords.words(&#x27;english&#x27;)]
  row.words = words_without_sw
  return row</code></pre><p id="c433b4db-fc9e-4f49-b7de-82be5f970bc7" class="">
</p><h3 id="408cfdf4-d978-4be1-a2e6-53e1cb578298" class="">Lemmatization</h3><p id="6cf44419-05d0-4738-9796-2ddcc309a4ab" class="">Then we lemmatized with the nltk function WordNetLemmatizer in order to gather words of same lemma. </p><pre id="cda5c0f7-7b17-4b05-8d57-ddeec1c7ffc7" class="code"><code>lemmatizer = WordNetLemmatizer()

def lemmatize(row):
  lemmatized_words = [lemmatizer.lemmatize(word) for word in row.words]
  row.words = lemmatized_words
  return row</code></pre><p id="46195a12-c094-4d6e-8fb0-961dc5ae8b21" class="">After that we dropped all words that were only present less that 5 times in the whole dataset because they were a lot but they weren’t significative at all.</p><p id="6b6d38fc-6464-4624-9690-ab96ee532cc6" class="">
</p><h3 id="e76dd377-53c3-43f3-9081-60c265884602" class="">TF-IDF</h3><p id="de134b29-972b-4886-b110-2e45c5ed04d2" class="">We had our words tokenized and lemmatized but we wanted to clean the words list a bit more in order to keep only the most relevant words. To do so, we computed a TF-IDF algorithm so that we could drop non-significative words wether they were to widespread or not enough. </p><p id="7665d6c1-b67d-49e8-b5b2-9e8cd85456e6" class="">
</p><p id="b0bdeb52-f839-4135-a6f5-134883bc70a4" class="">For that, we applied the TfidfVectorizer function from sklearn library to our word lists.</p><p id="14a39454-30ac-4646-ae8c-1eb28fedad4b" class="">
</p><p id="394278b4-f2ae-48c1-844f-73ff60d0bea6" class="">We first gathered all words in 5 groups, one for each rate, in order to use the result as the input of TF-IDF vectorizer:</p><pre id="65204563-df35-4e8c-ae43-27424e9694f5" class="code"><code>def get_all_words(data):
  words = data.words.sum()
  return list(words)

all_corpus=[]

for i in range(1, 6):
  words_note = train_data[train_data.note==i][&#x27;words&#x27;]
  all_corpus.append(&quot; &quot;.join(get_all_words(train_data[train_data.note == i])))</code></pre><p id="197208cd-6882-423a-ad61-b70394da0fba" class="">
</p><p id="ddca9fe6-3707-46eb-8bef-0746eba4fe7e" class="">And then we applied the vectorizer:</p><pre id="b7cd36fb-8e52-4656-832d-81c89aa25abf" class="code"><code>from sklearn.feature_extraction.text import TfidfVectorizer

def get_tfidf(note, n_words):
  # create a TfidfVectorizer object
  tfidf_vectorizer = TfidfVectorizer()

  # compute the TF-IDF values for each word in the text data
  tfidf_matrix = tfidf_vectorizer.fit_transform(all_corpus)
  terms = tfidf_vectorizer.get_feature_names_out()
  
  df = pd.DataFrame(tfidf_matrix.todense(), columns=terms).T

  dictionary=dict(sorted(df[note-1].to_dict().items(), key=lambda x: x[1], reverse=True))

  dictionary={k: dictionary[k] for k in list(dictionary)[:n_words]}
  
  return (list(dictionary.keys()), df)</code></pre><p id="5d887578-b23d-4cba-8784-4341110a6eb7" class="">
</p><p id="4b0a36a2-051d-416e-9a31-dc032cab9343" class="">We obtained matrices with 5 significativity indicators for each word (one for each rate):</p><figure id="bd70ff9f-dd5f-4651-97ae-d56b3e15a1f3" class="image" style="text-align:left"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%202.png"><img style="width:472px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%202.png"/></a></figure><p id="54cd7475-c1d8-4356-ab4f-0dd758f14957" class="">
</p><p id="71abfa9e-9b56-4a43-a87d-3691d8e102fd" class="">Unfortunatly the top words were very similar for each rate so we had to add another function to drop words present for each rate because they weren’t significative at all. </p><p id="21d7481d-418a-4809-901a-d32d7bd582c0" class="">We stored the new word lists in the “final_words” column of our dataframe.</p><p id="cce348b4-7f79-4dfe-9838-c672362edb84" class="">
</p><p id="8efb64a7-c5ca-4ee0-9fa6-f5db8ca4dda7" class="">
</p><h1 id="5ccdc491-c1fe-4b25-9b1b-7be4f07179c1" class="">Data Exploration</h1><p id="f1b9fb98-82d3-441e-bb43-4a79ee24ec5c" class="">
</p><h3 id="7b578cdc-c3d5-435d-8bd1-e647e7fb93bb" class="">Reviews per insurance</h3><figure id="60671f9c-4f46-4c2c-b874-d74f497b183e" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%203.png"><img style="width:1176px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%203.png"/></a></figure><p id="cb08cc74-9c12-41d4-adf5-c77ef1427057" class="">We can see that the 2 most represented insurances are Direct Assurance (appr. 5000 comments) and L&#x27;olivier Assurance (appr. 4000 comments). The other ones have no more that 1000 comments.</p><p id="dad77e4f-191a-48fd-b8dd-8731a49bc377" class="">
</p><h3 id="4c77cb2a-0dae-48da-aedf-372b2cbecad8" class=""><strong>Ranking based on insurance&#x27;s average rate</strong></h3><figure id="6e19d63b-309e-4a5e-9ab3-ea4eb56b3603" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%204.png"><img style="width:1157px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%204.png"/></a></figure><p id="55a5d6b0-dee9-4ad1-a51a-916245034d3b" class="">
</p><h3 id="d3af9d8c-330e-466c-9baf-4d541a8c9e35" class=""><strong>Product repartition</strong></h3><p id="84a89812-2958-48f6-ac04-86a13b3aafed" class="">
</p><figure id="cd4f4de2-3442-4ff4-b5bb-2dedf69e02aa" class="image" style="text-align:left"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%205.png"><img style="width:449px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%205.png"/></a></figure><p id="24a985ed-4b28-43ba-9e20-95395d1872f9" class="">We can clearly see that the vast majority of reviews are about auto insurance.</p><p id="988b5a5f-1068-4a6b-83ed-65445f9cbe23" class="">
</p><h3 id="6b80d64b-7a47-450e-81eb-0348754e8e2e" class=""><strong>Rating evolution per insurance</strong></h3><p id="178ea709-5ad5-4a99-8f29-4200f10602aa" class="">We wanted to use the dates to have a closer look on the rating evolution of each insurance so visualize the trends for each insurance. For example, for Direct Assurance, we can clearly see that the service have improved since 2017 according to customers’ reviews.</p><figure id="28641fdd-d192-4459-bdee-bb5d68302086" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%206.png"><img style="width:1157px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%206.png"/></a></figure><p id="9f4ec60a-b227-4e10-adc8-e752e293aa23" class="">
</p><h3 id="4eef1646-6c13-4a3e-9155-2b5a9af1007c" class="">Wordclouds </h3><p id="30a84b9e-85a6-4a20-9c16-068be6c6c8c7" class=""> </p><p id="580896db-4755-45fa-a7ea-ca4571cff59b" class="">Without tf-idf, the wordclouds are very close and the top words are not significative at all because they are omnipresent for all rates:</p><figure id="846af741-cba5-458a-a733-b3fa7175ca7c" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%207.png"><img style="width:851px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%207.png"/></a></figure><p id="f5a524ea-ca3c-42bf-927f-535e53a68fd7" class="">
</p><p id="b39cc65f-80c6-44e2-bab3-7a7af0b2b18f" class="">
</p><p id="d218159e-0a5d-4a17-9c87-94970610d8f6" class="">On the contrary, after apllying the tf-idf, we can clearly see the differences between high rates and low rates. In low rates we have words like “flee” or “terminated” which are very negative words, whereas in high rates we have ‘”fast”, “efficient” which are positive ones.</p><figure id="5f95fc6f-dbf6-4bda-9dc1-145fb7b681b6" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%208.png"><img style="width:851px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%208.png"/></a></figure><p id="5ada606b-dd76-4683-bcb4-ca59809f61a9" class="">
</p><p id="5535b908-f528-4b50-9b3e-d9adf7b70dfb" class="">
</p><h1 id="e6729482-351f-4b39-a657-60a629f3f77e" class="">Unsupervised Learning</h1><p id="1fd48d0f-c357-4f38-a69a-521bdbe85b1b" class="">
</p><p id="27e660d5-0cd2-410a-981d-fac5accd3837" class="">For this part, we’ve tried different methods to compare and keep the one with the best results:</p><ul id="3cf83b2d-6f83-4928-a18a-801a4ee0c675" class="bulleted-list"><li style="list-style-type:disc">Kmeans </li></ul><ul id="f009822a-8cee-468c-a27a-a0a9a3e29c74" class="bulleted-list"><li style="list-style-type:disc">LDA </li></ul><p id="b20989f4-6392-400e-ab78-4c4c549a9d00" class="">
</p><p id="3ae358d2-86d3-4bb4-a4fd-4feee7d0e518" class="">We first tried to highlight 2 clusters representing wether the reviews are positive or negative. Then, with the LDA, we tried to make 5 clusters like the 5 possible rates. </p><p id="13890b40-7f5f-4a71-9fb4-5ad2a3fd1971" class="">
</p><h2 id="c0b2a0c1-4ecc-4ff5-8c29-a74d0e17d905" class="">Kmeans </h2><p id="c9c50908-985d-4ad0-a6be-1087c7fa2f2b" class="">
</p><p id="445265b1-fc4e-4897-b144-155731626993" class="">First, to compute a Kmeans model, we had to vectorize our reviews. We created a Word2vec model with the gensim library to convert all words to 100-dim vectors:</p><p id="f0191b2a-e60f-4bb6-87ec-86531e949cdc" class="">
</p><pre id="ca202dcd-e1ee-4ebf-b294-e8d149e08e37" class="code"><code>from gensim.models import Word2Vec

model = Word2Vec(sentences=train_data.final_words, size=100)</code></pre><p id="64d7a7d9-da87-43d9-ae7d-727e788f2797" class="">
</p><p id="3ff2f654-de05-4a03-afe0-94afe7c43b7d" class="">Then, having a vector for each word, we vectorized the final_words lists in order to have one vector per review:</p><pre id="faeabc85-bf1e-449e-ac23-0db39fbb6c7e" class="code"><code>def vectorize_avis(avis):
  return np.mean(np.array([model.wv[ing] for ing in avis.final_words]), axis=0 )

def apply_vectorization(row):
  return vectorize_avis(row)

train_data[&quot;avis_vector&quot;] = train_data.progress_apply(apply_vectorization, axis=1)</code></pre><p id="2efe4dca-e8a1-4751-a5cd-e3354f95166f" class="">
</p><p id="fc2fe18a-7a98-491a-b928-35b4bae65d89" class="">With these vectors, we computed our Kmeans model with the sklearn library in order to get 2 clusters from the dataset:</p><pre id="f2163d3e-4abb-4d93-83ce-8e97b556b167" class="code"><code>from sklearn.cluster import KMeans

kmeans_scale = KMeans(n_clusters=2, random_state=42).fit(list(train_data.avis_vector))
labels_scale = kmeans_scale.labels_
clusters_scale = pd.concat([train_data.avis_vector, pd.DataFrame({&#x27;cluster_scaled&#x27;:labels_scale})], axis=1)</code></pre><p id="1282c952-de95-46c8-8bc1-99efc777713f" class="">
</p><p id="20b1b396-37d1-48a8-b04a-94d89c4c16f7" class="">With this model, we clusterised our reviews and compared the clusters to the real ones (based on ratings):</p><figure id="ea572e89-5506-45aa-b6cc-41c583f60d93" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%209.png"><img style="width:737px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%209.png"/></a></figure><p id="30fbbc7e-e920-498e-ab15-a80757388cd3" class="">
</p><p id="27d6f41f-fa43-4011-bbcc-341a52626bec" class="">This was what we obtained :</p><figure id="2e288797-3977-4126-a6a7-02f9a7a6a34c" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2010.png"><img style="width:1106px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2010.png"/></a></figure><p id="7f26e9fb-06bb-41f6-ba89-b89671a6bd05" class="">We can see that the clusterisation has been well done by the Kmeans. In fact, for negative predictions, the vast majority of real rates are lower than 3 when they are higher for positive predictions.</p><p id="887ce837-0940-46a3-a06f-593ec965ed7d" class="">
</p><p id="5e362676-1028-4107-aac3-60c8bbfcbb23" class="">Here is the confusion matrix we had:</p><figure id="18b45214-7954-4e77-bb80-0de172ef7c52" class="image" style="text-align:left"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2011.png"><img style="width:308px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2011.png"/></a></figure><p id="7fa1e501-5d02-4711-a89d-064a2559dad3" class="">
</p><p id="41173f30-987b-4517-8bcf-4d62de2dfe79" class="">We had 87% of good predictions so the Kmeans worked well for this binary clusterization. But it was not accurate at all when we tried with 5 clusters.</p><p id="312b985e-af17-4816-86b3-9190d107d89f" class="">
</p><p id="fcddc039-04c6-4ba1-a97c-d5be20918eca" class="">We then applied a PCA to visualize the clusterization:</p><figure id="83a74989-4791-4947-beb2-7a9e4ed22496" class="image" style="text-align:left"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2012.png"><img style="width:339px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2012.png"/></a></figure><p id="a968876d-ff8f-428e-aa90-35014e7935ee" class="">
</p><h2 id="74b0f3e9-8214-44a1-bc77-12b1d4edd1bf" class="">LDA </h2><p id="715f83e3-9bbe-49a6-b567-50e61422b9bd" class="">
</p><h3 id="67e48226-36ce-4077-ac65-0dafe672b062" class="">Clusterize positive and negative reviews </h3><p id="5bd69858-cb29-4bd6-b535-a680582e3749" class="">
</p><p id="2a303695-9c58-42f1-8ff4-8ee3b2f075f4" class="">First, we’ve computed a LDA model to clusterize in 2 groups, like we did for the Kmeans implementation. To do so we used the LdaMulticore function from the gensim library:</p><pre id="b676d372-cfa3-47de-bae8-2c2410f1bac6" class="code"><code>data = train_data.copy()
dictionary = corpora.Dictionary(data.final_words)
corpus = [dictionary.doc2bow(cleandoc) for cleandoc in data.final_words.to_list()]
ldamodel_pos_neg = models.LdaMulticore(corpus, num_topics=2, id2word = dictionary, passes=3)
clear_output()</code></pre><p id="81ea0465-8054-48b5-91c2-1a3303e04102" class="">
</p><p id="847c089c-a94a-4c12-a18b-49873608db82" class="">Then we used the pyLDAvis library to take a look at the top topics of the two clusters. </p><figure id="b3fdf76a-bf7b-45e9-8cc1-4aa9873f478f" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2013.png"><img style="width:1206px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2013.png"/></a></figure><p id="ab23d836-d23b-408c-b732-79c3fa358612" class="">For the first cluster, that appears to regroup more negative reviews, the top topics are letter, flee, terminated and reimburse. This is interesting because the main topic is “letter”, that can show there is a problem with the way insurances manage the letters and there is maybe a lack of answers to them.</p><p id="26a41ea0-6db6-49f3-99a6-82cc1944ab3c" class="">
</p><figure id="19ffaf1b-6488-4061-a798-3ede03438ba9" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2014.png"><img style="width:1200px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2014.png"/></a></figure><p id="23c127a8-1e74-48d7-9123-56f30bd51e81" class="">On the other side, for the second cluster that correspond to good reviews, the top topics are fast, quick, efficient and speed. As an interpretation we can say that customers put a lot of importance on the speed of insurances’ claim’s handling.</p><p id="a03312c8-9678-4bea-a776-29f77fc1e176" class="">
</p><h3 id="e2df0c89-a109-4686-9359-25f8e396b426" class=""><strong>Make 5 clusters to get principal topics of each notation</strong></h3><p id="4fe4c768-7310-4389-ab1a-89995c51ffee" class="">
</p><p id="ea6efac6-8dfe-44c5-88fa-70c1445b7330" class="">When we tried to extract 5 clusters from the LDA model, it was a bit more confused and 3 clusters were superimposed. </p><p id="d47ea87c-3976-4c48-8016-b5cee6d8ef79" class="">
</p><figure id="ca79d426-0508-4426-8e11-a3c8291e911a" class="image"><a href="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2015.png"><img style="width:1207px" src="NLP%20Project%202%20insurance%20reviews%20e9e87e0c4ad04269af5418a78390957a/Untitled%2015.png"/></a></figure><p id="021a2976-818b-448f-9f0d-057c92e17929" class="">
</p><p id="3a8cf92a-83de-408c-b4b4-1aa43a09b84a" class="">
</p><h1 id="a7c8d94c-e8f6-4f70-9d09-9fc01ba12f00" class="">Supervised learning</h1><p id="2e2e51e0-d65e-4086-be00-a5dfbe546f97" class="">
</p><p id="68cddee4-f878-4d97-a85b-7ea19c36c939" class="">To estimate the rate associated with each review, we decided to try a few models and compare their accuracies to select the best one for predicting our test set rates. We tries these 3 models:</p><ul id="771fb294-3537-494c-8ca4-3eadc60671b5" class="bulleted-list"><li style="list-style-type:disc">Linear Regression</li></ul><ul id="0a064ffe-9df8-4973-8eca-64520f7c65c5" class="bulleted-list"><li style="list-style-type:disc">Random Forest Regressor </li></ul><ul id="90d01b32-f394-4510-b261-ec1c4b1504e7" class="bulleted-list"><li style="list-style-type:disc">Random Forest Classifier </li></ul><p id="4b0998eb-2b17-486f-b898-697f690ad75d" class="">
</p><h2 id="8eb4b999-9453-4831-b8c4-2e15cb44c78d" class="">Linear Regression </h2><p id="e0d0f2dd-20cd-48ca-811d-1fe4fadce223" class="">
</p><p id="9a2eee1a-249e-4dbf-ac47-2f11209e7855" class="">First, we had to split our train dataset into train set and validation set. We also decided to add 3 fields to the dataset (product id, character length and word length), as we calculated them earlier in the pre-process.</p><p id="fce4557b-37eb-45c4-914d-707a707e1f3a" class="">
</p><pre id="8e69d022-e862-407e-ad74-677f84c4e283" class="code"><code>X = pd.DataFrame.from_records(train_data.avis_vector, columns=list(map(str, range(100))))
X = pd.concat([X, train_data[[&quot;product_cat&quot;, &quot;char_length&quot;, &quot;word_length&quot;]].reset_index(drop=True)], axis=1)


X_train,X_test,y_train,y_test=train_test_split(X,train_data.note,test_size=0.3)</code></pre><p id="fa8f6920-5149-48d7-8cc9-35495f2da7a6" class="">
</p><p id="1764cb1b-75a9-4534-88cf-555c7aa3eb93" class=""> Then, we created and fitted our Linear Regressor withthe sklearn library.</p><pre id="b17e4f22-3c5a-4add-b823-7aa9812d8d42" class="code"><code>LR = linear_model.LinearRegression()
LR = LR.fit(X_train, y_train)
LR_pred_train =LR.predict(X_train)
LR_pred_test =LR.predict(X_test)</code></pre><p id="c8ed430b-2a7a-4419-a434-5e41352b622c" class="">
</p><p id="ab148230-6f3f-458b-a55d-f3686a5837ea" class="">Finally we obtained these metrics:</p><pre id="1b9a49ff-36a4-46d6-8042-dd556375ac98" class="code"><code>R2: 0.6684936210782602, RMSE: 0.8891382971305557, accuracy: 0.4355361513527816
R2: 0.6644433522516083, RMSE: 0.8881941729649485, accuracy: 0.4412037037037037</code></pre><p id="c630677d-47a3-49c0-82b4-9838c96d4b0f" class="">
</p><p id="19ac9e90-6a6c-4c3f-a845-362880a2f6ca" class="">After that, we tried the same model but with a PCA to reduce the number of parameters and add more weight to the 3 fields we added, but the results were not better:</p><pre id="08cd7134-93a9-4156-bdcb-dd4b9322ac93" class="code"><code>R2: 0.6667387546788862, RMSE: 0.8919238831143769, accuracy: 0.4301117946682543
R2: 0.6520818220350554, RMSE: 0.9036107268503469, accuracy: 0.42345679012345677</code></pre><p id="1e5bf35a-8eb0-451e-90dd-b8e6ff68156b" class="">
</p><p id="4197f671-e767-4ebe-94b6-a3ce4b771a90" class="">
</p><h2 id="67719dbd-77c7-4217-98d9-e602b1b9739f" class="">Random Forest Regressor </h2><p id="d2c816ef-4cd2-44da-91f9-d01c451a53f6" class="">
</p><p id="2a8dead1-6437-493f-bd15-5959eb11d959" class="">To compute a Random Forest Regressor, we used the RandomForestRegressor function of the sklearn library and we previously applied a GridSearch on the train set to get the best parameters for this model. </p><p id="5a78d811-785e-4b9d-bfcd-d821f706edee" class="">
</p><pre id="6bcb824b-c765-4102-b760-f24d09e778f7" class="code"><code>randomForestAlgo = RandomForestRegressor()


param = {&#x27;n_estimators&#x27; : [int(x) for x in np.linspace(start=80,stop=100, num=10)], 
         &#x27;max_depth&#x27; : [90,100],
        }

gridSearch_RandomForest=GridSearchCV(randomForestAlgo,param,scoring=&#x27;r2&#x27;,cv=5)
gridSearch_RandomForest.fit(X_train,y_train)</code></pre><p id="df852925-431f-41ae-90e9-a0eeaaecc6e3" class="">
</p><p id="82efc257-eb65-4830-9a50-dde395283815" class="">With the parameters we obtained, we compute the Random Forest Regressor, and we obtained these results:</p><pre id="710a24f4-af05-46f8-b9ff-1d4ac5e32102" class="code"><code>Mean Squared Error: 0.5840339362030563
Accuracy:  0.4842592592592593</code></pre><p id="ff8fdab7-cd76-4f5f-b14d-ab07fefa5fee" class="">Quite better than the LinearRegression</p><p id="0fad447a-2a79-4647-9f03-b7053a10d2f2" class="">
</p><p id="d8f3f9f3-bcb3-4edc-92ce-2df497ac4ed7" class="">
</p><h2 id="ced1a4ae-3eb7-4055-83c7-254a540b0f66" class="">Random Forest Classifier</h2><p id="beeb30e2-bf44-47eb-b6bf-5016ef5ee3c9" class="">
</p><p id="a5af42db-4c80-4b9d-8829-5d84db981529" class="">Finally, we tried a Random Forest Classifier with the sklearn library:</p><pre id="ab9ca08b-5f43-4bd4-ae72-4b34e9094f46" class="code"><code>randomForestC= RandomForestClassifier(max_depth=10, n_estimators=100)
randomForestC.fit(X_train,y_train)
randomForestC_testScore=randomForestC.score(X_test,y_test)</code></pre><p id="59c198d3-1186-43b3-a909-5fe027ff9f98" class="">
</p><p id="b823a1a7-9a29-4510-af7b-50fd40a87663" class="">This is what we obtained:</p><pre id="286d5b0d-7fb9-43a6-a896-740b84a17ebb" class="code"><code>Mean Squared Error: 0.874537037037037
Accuracy:  0.5413580246913581</code></pre><p id="c2f24552-80f8-4eb8-86d5-f8e99e34fb8a" class="">
</p><p id="d2074a8b-3d35-4767-b4ca-d21a53e4578f" class="">This is the best accuracy we obtained so we’ll keep it for test dataset predictions.</p><p id="e07e6374-9f1a-4697-9848-313a269259ab" class="">
</p><p id="0efb460c-df37-408d-a178-e144ea090ac5" class="">
</p><h1 id="3351a0b9-32a1-4fbd-aa2d-33af0998e4a7" class="">Test set predictions </h1><p id="887feb17-63fe-4afd-923a-15afe4242b96" class="">
</p><p id="7cd4e1af-3a46-4680-89c2-e68f6c11c01c" class="">First, we had to compute the whole data processing we did for the train set. Previously, we also had to traduce all the test reviews.</p><p id="26c630a9-0da4-4ac7-9d49-c5354253bce6" class="">Then, we applied the Random Forest Classifier to do the predictions and we saved it on a csv file with the column “note”” containing all estimated rates.</p></div></article></body></html>